p8105_hw2_sjk2254
================
Sharon Kulali
2023-10-02

``` r
# loading the needed packages

library(tidyverse)
library(readxl)
```

## Problem 1

``` r
# importing the pols data

pols_df = read_csv("data/pols-month.csv")
```

``` r
# creating a new month variable

month_df = 
  tibble(
    month_num = 1:12,
    month_abb = month.abb,
    month = month.name
  )
```

``` r
# cleaning the data

pols_df =
  pols_df |> 
  separate("mon", c("year", "month", "day"), sep = "-")|> 
  mutate(month = as.numeric(month)) |>
  mutate(year = as.numeric(year)) |> 
  mutate(
    month = case_match(
      month,
      01 ~ "january",
      02 ~ "february",
      03 ~ "march",
      04 ~ "april",
      05 ~ "may",
      06 ~ "june",
      07 ~ "july",
      08 ~ "august",
      09 ~ "september",
      10 ~ "october",
      11 ~ "november",
      12 ~ "december"
    )) |> 
  mutate(president = recode(prez_gop, "0" = "dem", "1" = "gop", "2" = "gop")) |>
  left_join(x = _, y = month_df) |> 
  select(year, month, everything(), -day, -starts_with("prez"))
```

``` r
# importing the snp data set

snp = read_csv("data/snp.csv", col_types = cols(date = col_date(format = "%m/%d/%y")))
```

``` r
# cleaning the data

snp =
  snp |> 
  separate(date, into = c("year", "month_num", "day"), convert = TRUE) |>
  mutate(
    year = if_else(year > 2023, year - 100, year)) |> 
  left_join(x = _, y = month_df) |> 
  select(year, month, close)
```

``` r
# importing the unemployment data set

unemployment = read_csv("data/unemployment.csv")
```

``` r
# cleaning the data

unemployment = unemployment |> 
  rename(year = Year) 

unemployment = unemployment |> 
  pivot_longer(
    Jan:Dec, 
    names_to = "month_abb",
    values_to = "unemployment"
  ) |> 
  left_join(x = _, y = month_df) |> 
  select(year, month, unemployment)
```

``` r
# joining all the datasets

df_538 = 
  left_join(pols_df, snp) |>
  left_join(x = _, y = unemployment)
```

The FiveThirtyEight data comes from a statistician who wrote a piece
about the difficulties in sciences with the first part focusing on
p-hacking. The data contained 6 datasets including a dataset about the
number of national politicians who are democratic or republican at any
given time (`pols_df`), one relating to the Standard & Poor’s stock
market index (`snp`), and one about unemployment rates
(`unemployment`).The `pols_df` data has 822 observations and 11
variables with information from years 1947 to 2015. The `snp` data has
787 observations and 3 variables, ranging from years 1950 to 2015. The
`unemployment` data has 816 observations and 3 variables ranging from
years 1948 to 2015.

## Problem 2

### Mr. Trash Wheel Sheet

``` r
# importing mr. trash dataset

mt_df = readxl::read_excel("trash_wheel_data_updated.xlsx", sheet = 1, range = "A2:N587") 
```

``` r
# cleaning up the dataset

mt_df =
  janitor::clean_names(mt_df) |> 
  drop_na(dumpster)|> 
  mutate(
    year = as.numeric(year),
    vessel_name = "mr trash")
```

``` r
# updating the homes_powered variable

mt_df = 
  mutate(mt_df, homes_powered = (weight_tons*500)/30)
```

### Professor Trash Wheel Sheet

``` r
# importing professor trash dataset

pt_df = readxl::read_excel("trash_wheel_data_updated.xlsx", sheet = 2, range = "A2:M109")
```

``` r
# cleaning up the dataset

pt_df =
  janitor::clean_names(pt_df)|> 
  drop_na(dumpster)|> 
  mutate(vessel_name = "prof trash")
```

``` r
# updating the homes_powered variable

pt_df = mutate(pt_df, homes_powered = (weight_tons*500)/30)
```

### Gwynnda Trash Wheel Sheet

``` r
# importing gwynnda trash dataset

gt_df = readxl::read_excel("trash_wheel_data_updated.xlsx", sheet = 4, range = "A2:L159")
```

``` r
# cleaning up the dataset

gt_df =
  janitor::clean_names(gt_df)|> 
  drop_na(dumpster)|> 
  mutate(vessel_name = "gwyn trash") 
```

``` r
# updating the homes_powered variable

gt_df = mutate(gt_df, homes_powered = (weight_tons*500)/30)
```

### Combined

``` r
# creating a new data frame that combines all the data sets

trash_df = bind_rows(mt_df, pt_df, gt_df) |> 
  select(vessel_name, everything())
```

### Summary

Mr. Trash Wheel (`mt_df`), Professor Trash Wheel (`pt_df`), and Gwynnda
Trash Wheel (`gt_df`) are water vessels that remove trash from the Jones
Falls river in Baltimore, Maryland and dumps them into a dumpster site.
The data sets `mt_df`, `pt_df`, and `gt_df` include information about
the dumpster number, date of collection, amount of total litter, and the
specific litter type. The `mt_df` data has 584 observations and 15
variables with information ranging from years 2014 to 2023. The `pt_df`
data has 106 observations and 14 variables with information ranging from
years 2017 to 2023. The `gt_df` data has 155 observations and 13
variables with information ranging from years 2021 to 2023. The total
weight of trash collected by Professor Trash Wheel was 216.26 tons. The
total number of cigarette butts collected by Gwynnda Trash Wheel was
1.63^{4} tons. In the combined data set of all three trash vessels
(`trash_df`), there are 845 observations and 15 variables with an
average litter weight of 3.01 tons and information collected from years
2014 to 2023. Notably, the sports balls variable had 261 missing values.

## Problem 3

### Baseline Demographics

``` r
# importing the data set

baseline_df = read_csv("data/MCI_baseline.csv", skip = 1, col_names = TRUE)
```

``` r
# cleaning the data set

baseline_df =
 janitor::clean_names(baseline_df) |> 
  mutate(
    sex = case_match(
      sex,
      1 ~ "male",
      0 ~ "female"
    ),
    apoe4 = case_match(
      apoe4,
      1 ~ "carrier",
      0 ~ "non-carrier"
    )) 

baseline_df_tidy =
  baseline_df |> 
  mutate( age_at_onset = as.numeric(age_at_onset)
  ) |> 
  drop_na(age_at_onset)
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `age_at_onset = as.numeric(age_at_onset)`.
    ## Caused by warning:
    ## ! NAs introduced by coercion

### Summary

The dataset (`baseline_df`) describes basic demographic information of
participants in the study. The study aimed to understand the trajectory
of Alzheimer’s disease (AD) biomarkers by monitoring the development of
Mild Cognitive Impairement (MCI). The initial dataset contains
information about the variables in the first row thus when importing the
data, it had to be indicated that the first row should be skipped.
Overall, 483 participants were recruited and 97 developed MCI. The
average baseline age was 70 years old with 13.0434783 percent of women
in the study being a apoe4 carrier, the gene associated with a higher
risk of developing Alzheimer’s disease.

### Biomarker Values

``` r
# importing the data set

amyloid_df = read_csv("data/mci_amyloid.csv", skip = 1, col_names = TRUE)
```

``` r
# cleaning the data set

amyloid_df =
  janitor::clean_names(amyloid_df)|> 
  rename(id = study_id)
```

``` r
# joining the data sets


full_df = full_join(baseline_df_tidy, amyloid_df)
```

    ## Joining with `by = join_by(id)`

``` r
a_b_df = inner_join(baseline_df, amyloid_df)
```

    ## Joining with `by = join_by(id)`

``` r
final_df = inner_join(baseline_df_tidy, amyloid_df)
```

    ## Joining with `by = join_by(id)`
