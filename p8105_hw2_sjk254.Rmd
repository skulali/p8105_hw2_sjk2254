---
title: "p8105_hw2_sjk2254"
author: "Sharon Kulali"
date: "2023-10-02"
output: github_document
---

```{r setup, message = FALSE}
# loading the needed packages

library(tidyverse)
library(readxl)
```

## Problem 1

```{r, message = FALSE}
# importing the pols data

pols_df = read_csv("data/pols-month.csv")
```

```{r}
# creating a new month variable

month_df = 
  tibble(
    month_num = 1:12,
    month_abb = month.abb,
    month = month.name
  )
```

```{r, message = FALSE}
# cleaning the data

pols_df =
  pols_df |> 
  separate("mon", c("year", "month", "day"), sep = "-")|> 
  mutate(month = as.numeric(month)) |>
  mutate(year = as.numeric(year)) |> 
  mutate(
    month = case_match(
      month,
      01 ~ "january",
      02 ~ "february",
      03 ~ "march",
      04 ~ "april",
      05 ~ "may",
      06 ~ "june",
      07 ~ "july",
      08 ~ "august",
      09 ~ "september",
      10 ~ "october",
      11 ~ "november",
      12 ~ "december"
    )) |> 
  mutate(president = recode(prez_gop, "0" = "dem", "1" = "gop", "2" = "gop")) |>
  left_join(x = _, y = month_df) |> 
  select(year, month, everything(), -day, -starts_with("prez"))
```

```{r}
# importing the snp data set

snp = read_csv("data/snp.csv", col_types = cols(date = col_date(format = "%m/%d/%y")))
```

```{r, message = FALSE}
# cleaning the data

snp =
  snp |> 
  separate(date, into = c("year", "month_num", "day"), convert = TRUE) |>
  mutate(
    year = if_else(year > 2023, year - 100, year)) |> 
  left_join(x = _, y = month_df) |> 
  select(year, month, close)
```

```{r, message = FALSE}
# importing the unemployment data set

unemployment = read_csv("data/unemployment.csv")
```

```{r, message = FALSE}
# cleaning the data

unemployment = unemployment |> 
  rename(year = Year) 

unemployment = unemployment |> 
  pivot_longer(
    Jan:Dec, 
    names_to = "month_abb",
    values_to = "unemployment"
  ) |> 
  left_join(x = _, y = month_df) |> 
  select(year, month, unemployment)
```

```{r, message = FALSE}
# joining all the datasets

df_538 = 
  left_join(pols_df, snp) |>
  left_join(x = _, y = unemployment)
```

The FiveThirtyEight data comes from a statistician who wrote a piece about the difficulties in sciences with the first part focusing on p-hacking. The data contained 6 datasets including a dataset about the number of national politicians who are democratic or republican at any given time (`pols_df`), one relating to the Standard & Poorâ€™s stock market index (`snp`), and one about unemployment rates (`unemployment`).The `pols_df` data has `r nrow(pols_df)` observations and `r ncol(pols_df)` variables with information from years `r pols_df |> pull(year) |> min()` to `r pols_df |> pull(year) |> max()`. The `snp` data has `r nrow(snp)` observations and `r ncol(snp)` variables, ranging from years `r snp |> pull(year) |> min()` to `r snp |> pull(year) |> max()`. The `unemployment` data has `r nrow(unemployment)` observations and `r ncol(unemployment)` variables ranging from years `r unemployment |> pull(year) |> min()` to `r unemployment |> pull(year) |> max()`. 

## Problem 2

### Mr. Trash Wheel Sheet

```{r}
# importing mr. trash dataset

mt_df = readxl::read_excel("trash_wheel_data_updated.xlsx", sheet = 1, range = "A2:N587") 
```

```{r}
# cleaning up the dataset

mt_df =
  janitor::clean_names(mt_df) |> 
  drop_na(dumpster)|> 
  mutate(
    year = as.numeric(year),
    vessel_name = "mr trash")
```

```{r}
# updating the homes_powered variable

mt_df = 
  mutate(mt_df, homes_powered = (weight_tons*500)/30)
```

### Professor Trash Wheel Sheet

```{r}
# importing professor trash dataset

pt_df = readxl::read_excel("trash_wheel_data_updated.xlsx", sheet = 2, range = "A2:M109")
```

```{r}
# cleaning up the dataset

pt_df =
  janitor::clean_names(pt_df)|> 
  drop_na(dumpster)|> 
  mutate(vessel_name = "prof trash")
```

```{r}
# updating the homes_powered variable

pt_df = mutate(pt_df, homes_powered = (weight_tons*500)/30)
```

### Gwynnda Trash Wheel Sheet

```{r}
# importing gwynnda trash dataset

gt_df = readxl::read_excel("trash_wheel_data_updated.xlsx", sheet = 4, range = "A2:L159")
```

```{r}
# cleaning up the dataset

gt_df =
  janitor::clean_names(gt_df)|> 
  drop_na(dumpster)|> 
  mutate(vessel_name = "gwyn trash") 
```

```{r}
# updating the homes_powered variable

gt_df = mutate(gt_df, homes_powered = (weight_tons*500)/30)
```

### Combined

```{r}
# creating a new data frame that combines all the data sets

trash_df = bind_rows(mt_df, pt_df, gt_df) |> 
  select(vessel_name, everything())
```

### Summary 

Mr. Trash Wheel (`mt_df`), Professor Trash Wheel (`pt_df`), and Gwynnda Trash Wheel (`gt_df`) are water vessels that remove trash from the Jones Falls river in Baltimore, Maryland and dumps them into a dumpster site. The data sets `mt_df`, `pt_df`, and `gt_df` include information about the dumpster number, date of collection, amount of total litter, and the specific litter type. The `mt_df` data has `r nrow(mt_df)` observations and `r ncol(mt_df)` variables with information ranging from years `r mt_df |> pull(year) |> min()` to `r mt_df |> pull(year) |> max()`. The `pt_df` data has `r nrow(pt_df)` observations and `r ncol(pt_df)` variables with information ranging from years `r pt_df |> pull(year) |> min()` to `r pt_df |> pull(year) |> max()`. The `gt_df` data has `r nrow(gt_df)` observations and `r ncol(gt_df)` variables with information ranging from years `r gt_df |> pull(year) |> min()` to `r gt_df |> pull(year) |> max()`. The total weight of trash collected by Professor Trash Wheel was `r pt_df |> pull(weight_tons) |> sum()` tons. The total number of cigarette butts collected by Gwynnda Trash Wheel was `r gt_df |> filter(month == "July" & year == 2021) |> select(cigarette_butts) |> sum()` tons. In the combined data set of all three trash vessels (`trash_df`), there are `r nrow(trash_df)` observations and `r ncol(trash_df)` variables with an average litter weight of `r trash_df |> pull(weight_tons) |> mean() |> round(2)` tons and information collected from years `r trash_df |> pull(year) |> min()` to `r trash_df |> pull(year) |> max()`. Notably, the sports balls variable had `r trash_df |> pull(sports_balls) |> is.na() |> sum()` missing values.

## Problem 3

### Baseline Demographics

```{r, message = FALSE}
# importing the data set

baseline_df = read_csv("data/MCI_baseline.csv", skip = 1, col_names = TRUE, na = c(".","NA"))
```

```{r, message = FALSE}
# cleaning the data set

baseline_df =
 janitor::clean_names(baseline_df) |> 
  mutate(
    sex = case_match(
      sex,
      1 ~ "male",
      0 ~ "female"
    ),
    apoe4 = case_match(
      apoe4,
      1 ~ "carrier",
      0 ~ "non-carrier"
    )) |> 
      filter(current_age < age_at_onset | is.na(age_at_onset))
  
```

### Biomarker Values

```{r, message = FALSE}
# importing the data set

amyloid_df = read_csv("data/mci_amyloid.csv", skip = 1, col_names = TRUE)
```

```{r}
# cleaning the data set

amyloid_df =
  janitor::clean_names(amyloid_df)|> 
  rename(id = study_id) |> 
  pivot_longer(
    baseline:time_8,
    names_to = "time_elapse_years",
    values_to = "amyloid_ratio"
  )
```

```{r, eval = FALSE}
library(arsenal)

summary(comparedf(amyloid_df, baseline_df, by = "id"))
```


```{r}
# joining the data sets

final_df = inner_join(baseline_df, amyloid_df)
```

### Summary

The dataset (`baseline_df`) describes basic demographic information of participants in the study. The study aimed to understand the trajectory of Alzheimer's disease (AD) biomarkers by monitoring the development of Mild Cognitive Impairement (MCI). The dataste (`amyloid_df`) describes the changes over time in amyloid beta 42/40, which has been known to predict Alzheimer's disease. The initial datasets contains information about the variables in the first row thus when importing the data, it had to be indicated that the first row should be skipped and that the periods in the dataset should be considered NA's. Additionally, when tidying the `amyloid_df`, the number of rows increased because the time elapse information was moved to a column. There's about 8 individuals in the `baseline_df` who are not in the `amyloid_df`. Also, there are 16 individuals in the `amyloid_df`who were not in the `baseline_df`. I was able to find this using an available package from an online source. Overall, `r nrow(baseline_df)` participants were recruited after exclusions criteria (no MCI at baseline) and `r filter(baseline_df, age_at_onset != "NA") |> nrow()` participants developed MCI. The average baseline age was `r baseline_df |> pull(current_age) |> mean() |> round(2)` years old with `r (baseline_df |> filter(sex == "female" & apoe4 == "carrier") |> count()) / (nrow(baseline_df)) * 100|> round(0)` percent of women in the study being a apoe4 carrier, the gene associated with a higher risk of developing Alzheimer's disease. In the there `amyloid_df` data, there were `r nrow(amyloid_df)` observations and `r ncol(amyloid_df)` variables. When both data sets were combined (`final_df`), there were `r nrow(final_df)` observations and `r ncol(final_df)` variables.

```{r}
# exporting the results

write.csv(final_df, "data\\final_df.csv", row.names=TRUE)
```

